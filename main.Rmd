---
title: "main"
author: "Joshua Bela"
date: "May 4, 2020"
output:
  word_document: default
  pdf_document: default
---

import libraries
```{r}
library(magrittr)
library(caret)
library(xgboost)

```

import data
```{r}
data = read.csv('data/creditcard.csv')

```

For each variable, determine if it's eligible to be a predictor using T-tests.
The Pearson correlation must be non-zero, with a 95% confidence interval, for eligibility.
Create a new data set containing only eligible predictors.
```{r}
eligible_predictors = NULL
for(predictor in names(data)){
  confidence_interval = (cor.test(
    data$Class,
    data[,predictor],
    method = "pearson"
  ))$conf.int
  if(confidence_interval[1]*confidence_interval[2] > 0)
    eligible_predictors = c(eligible_predictors, predictor)
}

data0 = data[,eligible_predictors]

```

Data Engineering
Ensure all variables have the correct data types.
```{r}
data1 = data0
data1$Class = ifelse(
  data0$Class == 1,
  "fraud",
  "clean"
) %>% as.factor()

```

Apply regression using extreme gradient boosting in Caret.
```{r}
# create the tuning grid
xgb_grid_1 = expand.grid(
  nrounds = 10,
  eta=c(.3),
  gamma = c(0, .1, 1, 10),
  max_depth = seq(2, 10, by = 2),
  min_child_weight=1,
  subsample=0.5,
  colsample_bytree=0.5
)

# modify more settings
xgb_trcontrol_1 = trainControl(
  method = "cv",
  number = 2,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "all",                                                        # save losses across all models
  classProbs = TRUE,                                                           # set to TRUE for AUC to be computed
  summaryFunction = twoClassSummary,
  allowParallel = TRUE
)

# train the models on the data
xgb_train_1 = train(
  x = data1 %>% subset(select = -Class),
  y = data1$Class,
  trControl = xgb_trcontrol_1,
  tuneGrid = xgb_grid_1,
  method = "xgbTree",
  scale_pos_weight = 5
)

# print out the parameters for the best model
print(xgb_train_1$bestTune)

# print out parameters for all models in descending order of ROC
xgb_train_1$results[order(-xgb_train_1$results$ROC),]

# generate predictions using final model
predictions = predict(
  xgb_train_1,
  data1 %>% subset(select = -Class)
)

# generate confusion matrix based on predictions and true values
confusionMatrix(
  predictions,
  data1$Class
) %>% print()

```